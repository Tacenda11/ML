{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## 解题思路\n",
    "\n",
    "使用深度学习模型构建隐式推荐算法模型，并构建负样本，最终按照模型输出的评分进行排序，做出最终的推荐。具体可以分为以下几个步骤：\n",
    "\n",
    "- 步骤1：读取数据，对`用户`和`图书`进行编码；\n",
    "- 步骤2：利用训练集构建负样本；\n",
    "- 步骤3：使用Paddle构建打分模型；\n",
    "- 步骤4：对测试集数据进行预测；\n",
    "\n",
    "### 步骤1：读取数据集\n",
    "\n",
    "首先我们使用`pandas`读取数据集，并对数据的字段进行编码。这里可以手动构造编码过程，也可以使用`LabelEncoder`来完成。\n",
    "\n",
    "这一步骤的操作目的是将对`用户`和`图书`编码为连续的数值，原始的取值并不是连续的，这样可以减少后续模型所需要的空间。\n",
    "\n",
    "### 步骤2：构建负样本\n",
    "\n",
    "由于原始训练集中都是记录的是用户已有的图书记录，并不存在负样本。而在预测阶段我们需要预测用户下一个图书，此时的预测空间是用户对所有图书的关系。\n",
    "\n",
    "这里构建负样本的操作非常粗暴，直接是选择用户在训练集中没有图书。这里可以先使用协同过滤的思路来构建负样本，即将负样本是相似用户都没有记录的图书。\n",
    "\n",
    "### 步骤3：Paddle搭建打分模型\n",
    "\n",
    "这里构建使用Paddle构建用户与图书的打分模型，借助`Embedding`层来完成具体的匹配过程。这里用最简单的dot来完成匹配，没有构建复杂的模型。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1aa05bd8d08f44a5b8365c236994fd94d245b7de08ea459fb08b1ab9fe2d423e)\n",
    "\n",
    "### 步骤4：对测试集进行预测\n",
    "\n",
    "首先将测试集数据转为模型需要的格式，然后一行代码完成预测即可，然后转换为提交格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!cp /home/aistudio/data/data114712/test_dataset.csv ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共53424个用户，10000本图书，5869631条记录\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id\n",
       "0        0      257\n",
       "1        0      267\n",
       "2        0     5555\n",
       "3        0     3637\n",
       "4        0     1795"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train_dataset.csv')\n",
    "print('共{}个用户，{}本图书，{}条记录'.format(max(df['user_id'])+1, max(df['item_id'])+1, len(df)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.io import Dataset\n",
    "\n",
    "# 读取数据集\n",
    "df = pd.read_csv('train_dataset.csv')\n",
    "user_ids = df[\"user_id\"].unique().tolist()\n",
    "\n",
    "# 从新编码user 和 book，类似标签编码的过程\n",
    "# 此步骤主要为减少id的编码空间\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "\n",
    "book_ids = df[\"item_id\"].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}\n",
    "\n",
    "# 编码映射\n",
    "df[\"user\"] = df[\"user_id\"].map(user2user_encoded)\n",
    "df[\"movie\"] = df[\"item_id\"].map(book2book_encoded)\n",
    "\n",
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)\n",
    "\n",
    "user_book_dict = df.iloc[:].groupby(['user'])['movie'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "0        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "1        [115, 116, 117, 118, 119, 120, 21, 121, 122, 1...\n",
       "2        [30, 136, 171, 167, 165, 164, 159, 163, 166, 1...\n",
       "3        [260, 261, 262, 263, 264, 53, 265, 266, 267, 2...\n",
       "4        [333, 334, 335, 336, 337, 338, 339, 340, 341, ...\n",
       "                               ...                        \n",
       "53419    [2133, 941, 610, 2014, 559, 2973, 914, 1745, 1...\n",
       "53420    [6, 378, 119, 197, 41, 20, 45, 137, 46, 48, 36...\n",
       "53421    [5302, 2085, 2082, 2083, 2072, 2073, 2070, 208...\n",
       "53422    [6792, 8991, 8584, 326, 5248, 4994, 6739, 6738...\n",
       "53423    [7626, 6201, 6186, 8161, 8158, 8159, 5961, 609...\n",
       "Name: movie, Length: 53424, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user 与 电影的对应关系\n",
    "user_book_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 构造负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 随机挑选数据集作为负样本，负样本只需要对没有看的电影进行随机采样\n",
    "neg_df = []\n",
    "book_set = set(list(book_encoded2book.keys()))\n",
    "for user_idx in user_book_dict.index:\n",
    "    book_idx = book_set - set(list(user_book_dict.loc[user_idx]))\n",
    "    book_idx = list(book_idx)\n",
    "    neg_book_idx = np.random.choice(book_idx, 100)\n",
    "    for x in neg_book_idx:\n",
    "        neg_df.append([user_idx, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 负样本的标签\n",
    "neg_df = pd.DataFrame(neg_df, columns=['user', 'movie'])\n",
    "neg_df['label'] = 0\n",
    "\n",
    "# 正样本的标签\n",
    "df['label'] = 1\n",
    "\n",
    "# 正负样本合并为数据集\n",
    "train_df = pd.concat([df[['user', 'movie', 'label']], \n",
    "                      neg_df[['user', 'movie', 'label']]], axis=0)\n",
    "\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "del df;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "#映射式(map-style)数据集需要继承paddle.io.Dataset\n",
    "class SelfDefinedDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y, mode = 'train'):\n",
    "        super(SelfDefinedDataset, self).__init__()\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'predict':\n",
    "            return self.data_x[idx]\n",
    "        else:\n",
    "            return self.data_x[idx], self.data_y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集，得到训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_df[['user', 'movie']].values, \n",
    "                                        train_df['label'].values.astype(np.float32).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (1,)\n",
      "[7268 6400] [1.]\n",
      "[5120, 2]\n",
      "[5120, 1]\n",
      "[5120, 2]\n",
      "[5120, 1]\n"
     ]
    }
   ],
   "source": [
    "traindataset = SelfDefinedDataset(x_train, y_train)\n",
    "# 测试数据集读取\n",
    "for data, label in traindataset:\n",
    "    print(data.shape, label.shape)\n",
    "    print(data, label)\n",
    "    break\n",
    "\n",
    "# 测试dataloder读取\n",
    "train_loader = paddle.io.DataLoader(traindataset, batch_size = 1280*4, shuffle = True)\n",
    "for batch_id, data in enumerate(train_loader):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "\n",
    "    print(x_data.shape)\n",
    "    print(y_data.shape)\n",
    "    break\n",
    "\n",
    "val_dataset = SelfDefinedDataset(x_val, y_val)\n",
    "val_loader = paddle.io.DataLoader(val_dataset, batch_size = 1280*4, shuffle = True)        \n",
    "for batch_id, data in enumerate(val_loader):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "\n",
    "    print(x_data.shape)\n",
    "    print(y_data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "\n",
    "# 定义深度学习模型\n",
    "class RecommenderNet(nn.Layer):\n",
    "    def __init__(self, num_users, num_movies, embedding_size):\n",
    "        super(RecommenderNet, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        weight_attr_user = paddle.ParamAttr(\n",
    "            regularizer = paddle.regularizer.L2Decay(1e-6),\n",
    "            initializer = nn.initializer.KaimingNormal()\n",
    "            )\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            weight_attr=weight_attr_user\n",
    "        )\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        \n",
    "        weight_attr_movie = paddle.ParamAttr(\n",
    "            regularizer = paddle.regularizer.L2Decay(1e-6),\n",
    "            initializer = nn.initializer.KaimingNormal()\n",
    "            )\n",
    "        self.movie_embedding = nn.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            weight_attr=weight_attr_movie\n",
    "        )\n",
    "        self.movie_bias = nn.Embedding(num_movies, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = paddle.dot(user_vector, movie_vector)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1231 11:42:31.823208   171 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 10.1\n",
      "W1231 11:42:31.828727   171 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1643/1643 [==============================] - loss: 0.4185 - precision: 0.7948 - 65ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "step 548/548 [==============================] - loss: 0.4051 - precision: 0.8289 - 59ms/step         \n",
      "Eval samples: 2803008\n",
      "Epoch 2/5\n",
      "step 1643/1643 [==============================] - loss: 0.3579 - precision: 0.8458 - 70ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoints/1\n",
      "Eval begin...\n",
      "step 548/548 [==============================] - loss: 0.3606 - precision: 0.8507 - 60ms/step         \n",
      "Eval samples: 2803008\n",
      "Epoch 3/5\n",
      "step 1643/1643 [==============================] - loss: 0.3271 - precision: 0.8656 - 65ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoints/2\n",
      "Eval begin...\n",
      "step 548/548 [==============================] - loss: 0.3350 - precision: 0.8645 - 59ms/step        \n",
      "Eval samples: 2803008\n",
      "Epoch 4/5\n",
      "step 1643/1643 [==============================] - loss: 0.3009 - precision: 0.8796 - 64ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoints/3\n",
      "Eval begin...\n",
      "step 548/548 [==============================] - loss: 0.3164 - precision: 0.8735 - 59ms/step        \n",
      "Eval samples: 2803008\n",
      "Epoch 5/5\n",
      "step 1643/1643 [==============================] - loss: 0.3069 - precision: 0.8894 - 65ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoints/4\n",
      "Eval begin...\n",
      "step 548/548 [==============================] - loss: 0.3002 - precision: 0.8790 - 60ms/step         \n",
      "Eval samples: 2803008\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "model = RecommenderNet(num_users, num_books, EMBEDDING_SIZE)\n",
    "\n",
    "model = paddle.Model(model)\n",
    "\n",
    "# 定义模型损失函数、优化器和评价指标\n",
    "optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.003)\n",
    "loss = nn.BCELoss()\n",
    "metric = paddle.metric.Precision()\n",
    "\n",
    "# # 设置visualdl路径\n",
    "log_dir = './visualdl'\n",
    "callback = paddle.callbacks.VisualDL(log_dir=log_dir)\n",
    "\n",
    "# 模型训练与验证\n",
    "model.prepare(optimizer, loss, metric)\n",
    "model.fit(train_loader, val_loader, epochs=5, save_dir='./checkpoints', verbose=1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Detect dataset only contains single fileds, return format changed since Paddle 2.1. In Paddle <= 2.0, DataLoader add a list surround output data(e.g. return [data]), and in Paddle >= 2.1, DataLoader return the single filed directly (e.g. return data). For example, in following code: \n",
      "\n",
      "import numpy as np\n",
      "from paddle.io import DataLoader, Dataset\n",
      "\n",
      "class RandomDataset(Dataset):\n",
      "    def __getitem__(self, idx):\n",
      "        data = np.random.random((2, 3)).astype('float32')\n",
      "\n",
      "        return data\n",
      "\n",
      "    def __len__(self):\n",
      "        return 10\n",
      "\n",
      "dataset = RandomDataset()\n",
      "loader = DataLoader(dataset, batch_size=1)\n",
      "data = next(loader())\n",
      "\n",
      "In Paddle <= 2.0, data is in format '[Tensor(shape=(1, 2, 3), dtype=float32)]', and in Paddle >= 2.1, data is in format 'Tensor(shape=(1, 2, 3), dtype=float32)'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step   42/7724 [..............................] - ETA: 58s - 8ms/stepstep  102/7724 [..............................] - ETA: 43s - 6ms/st\n",
      "step 7724/7724 [==============================] - 5ms/step        \n",
      "Predict samples: 9886184\n",
      "Predict begin...\n",
      "step 7723/7723 [==============================] - 4ms/step         \n",
      "Predict samples: 9884390\n",
      "Predict begin...\n",
      "step 7723/7723 [==============================] - 5ms/step        \n",
      "Predict samples: 9884674\n",
      "Predict begin...\n",
      "step 7722/7722 [==============================] - 5ms/step        \n",
      "Predict samples: 9883296\n",
      "Predict begin...\n",
      "step 7722/7722 [==============================] - 5ms/step        \n",
      "Predict samples: 9883657\n",
      "Predict begin...\n",
      "step 7722/7722 [==============================] - 4ms/step        \n",
      "Predict samples: 9883750\n",
      "Predict begin...\n",
      "step 7721/7721 [==============================] - 4ms/step        \n",
      "Predict samples: 9881710\n",
      "Predict begin...\n",
      "step 7721/7721 [==============================] - 4ms/step         \n",
      "Predict samples: 9882276\n",
      "Predict begin...\n",
      "step 7722/7722 [==============================] - 4ms/step        \n",
      "Predict samples: 9883007\n",
      "Predict begin...\n",
      "step 7723/7723 [==============================] - 4ms/step        \n",
      "Predict samples: 9885012\n",
      "Predict begin...\n",
      "step 7723/7723 [==============================] - 4ms/step        \n",
      "Predict samples: 9884635\n",
      "Predict begin...\n",
      "step 7724/7724 [==============================] - 4ms/step        \n",
      "Predict samples: 9885505\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 5ms/step         \n",
      "Predict samples: 9887310\n",
      "Predict begin...\n",
      "step 7722/7722 [==============================] - 4ms/step        \n",
      "Predict samples: 9883799\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 5ms/step        \n",
      "Predict samples: 9886890\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 5ms/step        \n",
      "Predict samples: 9887735\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 4ms/step        \n",
      "Predict samples: 9887762\n",
      "Predict begin...\n",
      "step 7726/7726 [==============================] - 4ms/step        \n",
      "Predict samples: 9888197\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 6ms/step        \n",
      "Predict samples: 9887347\n",
      "Predict begin...\n",
      "step 7726/7726 [==============================] - 5ms/step        \n",
      "Predict samples: 9888719\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 4ms/step        \n",
      "Predict samples: 9887990\n",
      "Predict begin...\n",
      "step 7726/7726 [==============================] - 4ms/step        \n",
      "Predict samples: 9889006\n",
      "Predict begin...\n",
      "step 7726/7726 [==============================] - 4ms/step        \n",
      "Predict samples: 9888147\n",
      "Predict begin...\n",
      "step 7725/7725 [==============================] - 4ms/step        \n",
      "Predict samples: 9887586\n",
      "Predict begin...\n",
      "step 7727/7727 [==============================] - 4ms/step        \n",
      "Predict samples: 9889405\n",
      "Predict begin...\n",
      "step 7727/7727 [==============================] - 4ms/step        \n",
      "Predict samples: 9890484\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 4ms/step        \n",
      "Predict samples: 9892153\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 4ms/step         \n",
      "Predict samples: 9892469\n",
      "Predict begin...\n",
      "step 7727/7727 [==============================] - 4ms/step        \n",
      "Predict samples: 9890479\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 4ms/step        \n",
      "Predict samples: 9892083\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 5ms/step        \n",
      "Predict samples: 9892682\n",
      "Predict begin...\n",
      "step 7728/7728 [==============================] - 5ms/step        \n",
      "Predict samples: 9891381\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 5ms/step        \n",
      "Predict samples: 9892177\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 4ms/step        \n",
      "Predict samples: 9892739\n",
      "Predict begin...\n",
      "step 7730/7730 [==============================] - 5ms/step        \n",
      "Predict samples: 9893775\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 5ms/step        \n",
      "Predict samples: 9892322\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 5ms/step        \n",
      "Predict samples: 9893108\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 4ms/step        \n",
      "Predict samples: 9893060\n",
      "Predict begin...\n",
      "step 7730/7730 [==============================] - 4ms/step        \n",
      "Predict samples: 9893915\n",
      "Predict begin...\n",
      "step 7730/7730 [==============================] - 4ms/step        \n",
      "Predict samples: 9893524\n",
      "Predict begin...\n",
      "step 7729/7729 [==============================] - 5ms/step        \n",
      "Predict samples: 9892847\n",
      "Predict begin...\n",
      "step 7732/7732 [==============================] - 4ms/step        \n",
      "Predict samples: 9896045\n",
      "Predict begin...\n",
      "step 7732/7732 [==============================] - 4ms/step        \n",
      "Predict samples: 9896004\n",
      "Predict begin...\n",
      "step 7731/7731 [==============================] - 4ms/step        \n",
      "Predict samples: 9895654\n",
      "Predict begin...\n",
      "step 7731/7731 [==============================] - 4ms/step        \n",
      "Predict samples: 9895271\n",
      "Predict begin...\n",
      "step 7732/7732 [==============================] - 4ms/step        \n",
      "Predict samples: 9896799\n",
      "Predict begin...\n",
      "step 7732/7732 [==============================] - 5ms/step        \n",
      "Predict samples: 9895716\n",
      "Predict begin...\n",
      "step 7730/7730 [==============================] - 4ms/step        \n",
      "Predict samples: 9894049\n",
      "Predict begin...\n",
      "step 7731/7731 [==============================] - 4ms/step        \n",
      "Predict samples: 9894682\n",
      "Predict begin...\n",
      "step 7733/7733 [==============================] - 4ms/step        \n",
      "Predict samples: 9897552\n",
      "Predict begin...\n",
      "step 7731/7731 [==============================] - 5ms/step        \n",
      "Predict samples: 9894785\n",
      "Predict begin...\n",
      "step 7733/7733 [==============================] - 4ms/step        \n",
      "Predict samples: 9896971\n",
      "Predict begin...\n",
      "step 7731/7731 [==============================] - 5ms/step        \n",
      "Predict samples: 9895377\n",
      "Predict begin...\n",
      "step 3277/3277 [==============================] - 5ms/step        \n",
      "Predict samples: 4194277\n"
     ]
    }
   ],
   "source": [
    "test_df = []\n",
    "with open('sub.csv', 'w') as up:\n",
    "    up.write('user_id,item_id\\n')\n",
    "\n",
    "# 模型预测步骤\n",
    "book_set = set(list(book_encoded2book.keys()))\n",
    "for idx in range(int(len(user_book_dict)/1000) +1):\n",
    "    # 对于所有的用户，需要预测其与其他书的打分\n",
    "    test_user_idx = []\n",
    "    test_book_idx = []\n",
    "    for user_idx in user_book_dict.index[idx*1000:(idx+1)*1000]:\n",
    "        \n",
    "        book_idx = book_set - set(list(user_book_dict.loc[user_idx]))\n",
    "        book_idx = list(book_idx)\n",
    "        test_user_idx += [user_idx] * len(book_idx)\n",
    "        test_book_idx +=  book_idx\n",
    "    \n",
    "    # 从剩余电影中筛选出标签为正的样本\n",
    "    test_data = np.array([test_user_idx, test_book_idx]).T\n",
    "    test_dataset = SelfDefinedDataset(test_data, data_y=None, mode='predict')\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, batch_size=1280, shuffle = False)        \n",
    "        \n",
    "    test_predict = model.predict(test_loader, batch_size=1024)\n",
    "    test_predict = np.concatenate(test_predict[0], 0)\n",
    "    \n",
    "    test_data = pd.DataFrame(test_data, columns=['user', 'book'])\n",
    "    test_data['label'] = test_predict\n",
    "    for gp in test_data.groupby(['user']):\n",
    "        with open('sub.csv', 'a') as up:\n",
    "            u = gp[0]\n",
    "            b = gp[1]['book'].iloc[gp[1]['label'].argmax()]\n",
    "            up.write(f'{userencoded2user[u]}, {book_encoded2book[b]}\\n')\n",
    "        \n",
    "    del test_data, test_dataset, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
